2023-11-16 12:54:52 | INFO | model_worker | Loading the model mplug_owl2_7b_448_qinstruct_preview_v0.1 on worker 356d62 ...
2023-11-16 12:54:53 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                      | 0/33 [00:00<?, ?it/s]
2023-11-16 12:54:53 | ERROR | stderr | Loading checkpoint shards:   3%|█▉                                                            | 1/33 [00:00<00:12,  2.53it/s]
2023-11-16 12:54:54 | ERROR | stderr | Loading checkpoint shards:   6%|███▊                                                          | 2/33 [00:00<00:12,  2.58it/s]
2023-11-16 12:54:54 | ERROR | stderr | Loading checkpoint shards:   9%|█████▋                                                        | 3/33 [00:01<00:11,  2.58it/s]
2023-11-16 12:54:54 | ERROR | stderr | Loading checkpoint shards:  12%|███████▌                                                      | 4/33 [00:01<00:11,  2.59it/s]
2023-11-16 12:54:55 | ERROR | stderr | Loading checkpoint shards:  15%|█████████▍                                                    | 5/33 [00:01<00:10,  2.60it/s]
2023-11-16 12:54:55 | ERROR | stderr | Loading checkpoint shards:  18%|███████████▎                                                  | 6/33 [00:02<00:10,  2.60it/s]
2023-11-16 12:54:56 | ERROR | stderr | Loading checkpoint shards:  21%|█████████████▏                                                | 7/33 [00:02<00:10,  2.58it/s]
2023-11-16 12:54:56 | ERROR | stderr | Loading checkpoint shards:  24%|███████████████                                               | 8/33 [00:03<00:09,  2.57it/s]
2023-11-16 12:54:56 | ERROR | stderr | Loading checkpoint shards:  27%|████████████████▉                                             | 9/33 [00:03<00:09,  2.57it/s]
2023-11-16 12:54:57 | ERROR | stderr | Loading checkpoint shards:  30%|██████████████████▍                                          | 10/33 [00:03<00:08,  2.59it/s]
2023-11-16 12:54:57 | ERROR | stderr | Loading checkpoint shards:  33%|████████████████████▎                                        | 11/33 [00:04<00:08,  2.60it/s]
2023-11-16 12:54:58 | ERROR | stderr | Loading checkpoint shards:  36%|██████████████████████▏                                      | 12/33 [00:04<00:08,  2.60it/s]
2023-11-16 12:54:58 | ERROR | stderr | Loading checkpoint shards:  39%|████████████████████████                                     | 13/33 [00:05<00:07,  2.61it/s]
2023-11-16 12:54:58 | ERROR | stderr | Loading checkpoint shards:  42%|█████████████████████████▉                                   | 14/33 [00:05<00:07,  2.61it/s]
2023-11-16 12:54:59 | ERROR | stderr | Loading checkpoint shards:  45%|███████████████████████████▋                                 | 15/33 [00:05<00:06,  2.62it/s]
2023-11-16 12:54:59 | ERROR | stderr | Loading checkpoint shards:  48%|█████████████████████████████▌                               | 16/33 [00:06<00:06,  2.61it/s]
2023-11-16 12:54:59 | ERROR | stderr | Loading checkpoint shards:  52%|███████████████████████████████▍                             | 17/33 [00:06<00:06,  2.60it/s]
2023-11-16 12:55:00 | ERROR | stderr | Loading checkpoint shards:  55%|█████████████████████████████████▎                           | 18/33 [00:06<00:05,  2.58it/s]
2023-11-16 12:55:00 | ERROR | stderr | Loading checkpoint shards:  58%|███████████████████████████████████                          | 19/33 [00:07<00:05,  2.56it/s]
2023-11-16 12:55:01 | ERROR | stderr | Loading checkpoint shards:  61%|████████████████████████████████████▉                        | 20/33 [00:07<00:05,  2.55it/s]
2023-11-16 12:55:01 | ERROR | stderr | Loading checkpoint shards:  64%|██████████████████████████████████████▊                      | 21/33 [00:08<00:04,  2.54it/s]
2023-11-16 12:55:01 | ERROR | stderr | Loading checkpoint shards:  67%|████████████████████████████████████████▋                    | 22/33 [00:08<00:04,  2.54it/s]
2023-11-16 12:55:02 | ERROR | stderr | Loading checkpoint shards:  70%|██████████████████████████████████████████▌                  | 23/33 [00:08<00:03,  2.57it/s]
2023-11-16 12:55:02 | ERROR | stderr | Loading checkpoint shards:  73%|████████████████████████████████████████████▎                | 24/33 [00:09<00:03,  2.58it/s]
2023-11-16 12:55:03 | ERROR | stderr | Loading checkpoint shards:  76%|██████████████████████████████████████████████▏              | 25/33 [00:09<00:03,  2.58it/s]
2023-11-16 12:55:03 | ERROR | stderr | Loading checkpoint shards:  79%|████████████████████████████████████████████████             | 26/33 [00:10<00:02,  2.59it/s]
2023-11-16 12:55:04 | ERROR | stderr | Loading checkpoint shards:  82%|█████████████████████████████████████████████████▉           | 27/33 [00:10<00:03,  1.86it/s]
2023-11-16 12:55:04 | ERROR | stderr | Loading checkpoint shards:  85%|███████████████████████████████████████████████████▊         | 28/33 [00:11<00:02,  2.14it/s]
2023-11-16 12:55:05 | ERROR | stderr | Loading checkpoint shards:  88%|█████████████████████████████████████████████████████▌       | 29/33 [00:11<00:01,  2.34it/s]
2023-11-16 12:55:05 | ERROR | stderr | Loading checkpoint shards:  91%|███████████████████████████████████████████████████████▍     | 30/33 [00:11<00:01,  2.45it/s]
2023-11-16 12:55:05 | ERROR | stderr | Loading checkpoint shards:  94%|█████████████████████████████████████████████████████████▎   | 31/33 [00:12<00:00,  2.50it/s]
2023-11-16 12:55:06 | ERROR | stderr | Loading checkpoint shards:  97%|███████████████████████████████████████████████████████████▏ | 32/33 [00:12<00:00,  2.52it/s]
2023-11-16 12:55:06 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████| 33/33 [00:13<00:00,  2.52it/s]
2023-11-16 12:55:06 | ERROR | stderr | Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████| 33/33 [00:13<00:00,  2.51it/s]
2023-11-16 12:55:06 | ERROR | stderr | 
2023-11-16 12:55:06 | WARNING | transformers.modeling_utils | Some weights of MPLUGOwl2LlamaForCausalLM were not initialized from the model checkpoint at teowu/mplug_owl2_7b_448_qinstruct_preview_v0.1 and are newly initialized: ['model.visual_abstractor.encoder.layers.0.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.4.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.1.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.4.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.5.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.2.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.0.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.3.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.1.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.5.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.2.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.3.crossattention.attention.k_pos_embed']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-11-16 12:55:06 | WARNING | transformers.modeling_utils | Some weights of MPLUGOwl2LlamaForCausalLM were not initialized from the model checkpoint at teowu/mplug_owl2_7b_448_qinstruct_preview_v0.1 and are newly initialized: ['model.visual_abstractor.encoder.layers.0.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.4.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.1.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.4.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.5.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.2.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.0.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.3.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.1.crossattention.attention.q_pos_embed', 'model.visual_abstractor.encoder.layers.5.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.2.crossattention.attention.k_pos_embed', 'model.visual_abstractor.encoder.layers.3.crossattention.attention.k_pos_embed']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-11-16 12:55:07 | ERROR | stderr | /home/ps/anaconda3/lib/python3.11/site-packages/gradio/blocks.py:889: UserWarning: api_name http_bot already exists, using http_bot_1
2023-11-16 12:55:07 | ERROR | stderr |   warnings.warn(f"api_name {api_name} already exists, using {api_name_}")
2023-11-16 12:55:07 | ERROR | stderr | /home/ps/anaconda3/lib/python3.11/site-packages/gradio/blocks.py:889: UserWarning: api_name add_text already exists, using add_text_1
2023-11-16 12:55:07 | ERROR | stderr |   warnings.warn(f"api_name {api_name} already exists, using {api_name_}")
2023-11-16 12:55:07 | ERROR | stderr | /home/ps/anaconda3/lib/python3.11/site-packages/gradio/blocks.py:889: UserWarning: api_name http_bot already exists, using http_bot_2
2023-11-16 12:55:07 | ERROR | stderr |   warnings.warn(f"api_name {api_name} already exists, using {api_name_}")
2023-11-16 12:55:07 | ERROR | stderr | /home/ps/anaconda3/lib/python3.11/site-packages/gradio/blocks.py:889: UserWarning: api_name add_text already exists, using add_text_2
2023-11-16 12:55:07 | ERROR | stderr |   warnings.warn(f"api_name {api_name} already exists, using {api_name_}")
2023-11-16 12:55:07 | INFO | stdout | Running on local URL:  http://0.0.0.0:7860
2023-11-16 12:55:20 | INFO | stdout | Running on public URL: https://7d234e76d156b22bb4.gradio.live
2023-11-16 12:55:20 | INFO | stdout | 
2023-11-16 12:55:20 | INFO | stdout | This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)
2023-11-16 12:58:58 | INFO | stdout | ('<|image|>Is this image clear?', <PIL.Image.Image image mode=RGB size=640x481 at 0x7FEB08F68C90>, 'Default')
2023-11-16 12:59:00 | WARNING | transformers.generation.utils | The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2023-11-16 12:59:00 | WARNING | transformers.generation.utils | The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2023-11-16 12:59:00 | WARNING | transformers.generation.utils | Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
2023-11-16 12:59:00 | WARNING | transformers.generation.utils | Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
2023-11-16 12:59:08 | INFO | stdout | ('<|image|>Is this image clear?', <PIL.Image.Image image mode=RGB size=640x481 at 0x7FEB3C3CD8D0>, 'Default')
2023-11-16 12:59:10 | INFO | stdout | ('<|image|>Is this image clear?', <PIL.Image.Image image mode=RGB size=640x481 at 0x7FEB3C3CD8D0>, 'Default')
2023-11-16 12:59:10 | INFO | stdout | ('<|image|>Rate the quality of the image.', <PIL.Image.Image image mode=RGB size=640x481 at 0x7FEB3C3CD8D0>, 'Default')
2023-11-16 12:59:10 | INFO | stdout | torch.Size([32000])
2023-11-16 12:59:10 | INFO | stdout | {'input_ids': [[1, 1781], [1, 6588], [1, 6460]], 'attention_mask': [[1, 1], [1, 1], [1, 1]]}
